name: Comparative Framework Benchmark

on:
  workflow_dispatch:
    inputs:
      iterations:
        description: "Number of benchmark iterations"
        required: false
        default: "5"
        type: string
      timeout:
        description: "Timeout per document in seconds"
        required: false
        default: "300"
        type: string
      branch:
        description: "Git branch to benchmark"
        required: false
        default: "main"
        type: string

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 240

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.inputs.branch || github.ref_name }}

      - name: Ensure benchmark harness exists
        run: |
          if [ ! -d tools/benchmark-harness ]; then
            echo "::error::tools/benchmark-harness not found on branch ${GITHUB_REF}. Run this workflow against v4-dev." >&2
            exit 1
          fi

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1 \
            pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: |
            uv.lock

      - name: Sync benchmark dependencies
        run: uv sync --group benchmark

      - name: Build benchmark harness
        run: cargo build --manifest-path tools/benchmark-harness/Cargo.toml --release

      - name: Run benchmark harness
        env:
          ITERATIONS: ${{ github.event.inputs.iterations }}
          TIMEOUT: ${{ github.event.inputs.timeout }}
        run: |
          set -euo pipefail
          OUTPUT_DIR="benchmark-results"
          rm -rf "${OUTPUT_DIR}"

          echo "Running benchmarks with ${ITERATIONS:-5} iterations and ${TIMEOUT:-300}s timeout..."

          cargo run \
            --manifest-path tools/benchmark-harness/Cargo.toml \
            --release \
            --bin benchmark-harness \
            -- run \
            --fixtures tools/benchmark-harness/fixtures \
            --frameworks "kreuzberg-native,docling,markitdown,unstructured,extractous-python" \
            --output "${OUTPUT_DIR}" \
            --iterations "${ITERATIONS:-5}" \
            --timeout "${TIMEOUT:-300}"

          ls -R "${OUTPUT_DIR}"

      - name: Summarize benchmark results
        run: |
          set -euo pipefail
          RESULTS_PATH="benchmark-results/results.json"
          if [ ! -f "${RESULTS_PATH}" ]; then
            echo "âš ï¸ No benchmark results found" >> "${GITHUB_STEP_SUMMARY}"
            exit 0
          fi

          uv run python - <<'PY'
          import json
          from pathlib import Path

          results_path = Path("benchmark-results/results.json")
          data = json.loads(results_path.read_text())

          summary_lines = [
              "## ðŸ“Š Comparative Benchmark Results",
              "",
              "| Framework | Files | Success | Failures | Mean Duration (ms) |",
              "|-----------|-------|---------|----------|--------------------|",
          ]

          from collections import defaultdict
          stats = defaultdict(list)
          for entry in data:
              stats[entry["framework"]].append(entry)

          for framework, entries in stats.items():
              files = len(entries)
              successes = sum(1 for e in entries if e.get("success"))
              failures = files - successes
              durations = [e["duration"]["secs"] * 1000 + e["duration"]["nanos"] / 1_000_000 for e in entries]
              mean_ms = sum(durations) / len(durations) if durations else 0.0
              summary_lines.append(f"| {framework} | {files} | {successes} | {failures} | {mean_ms:.2f} |")

          summary_text = "\n".join(summary_lines)
          print(summary_text)
          Path("benchmark-results/summary.md").write_text(summary_text + "\n")
          PY

          cat benchmark-results/summary.md >> "${GITHUB_STEP_SUMMARY}"

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: comparative-benchmark-${{ github.run_id }}
          path: benchmark-results/
          retention-days: 30
