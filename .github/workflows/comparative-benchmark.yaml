name: Comparative Framework Benchmark

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      iterations:
        description: "Number of iterations per test"
        required: false
        default: "3"
        type: string
      timeout:
        description: "Timeout per file in seconds"
        required: false
        default: "300"
        type: string

jobs:
  skip-on-push:
    if: github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - name: Skip benchmark on push
        run: echo "Comparative benchmarks only run on manual dispatch"

  framework-benchmark:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        framework:
          - kreuzberg_sync
          - kreuzberg_async
          - docling
          - markitdown
          - unstructured
          - extractous

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: |
            uv.lock
            benchmarks/uv.lock

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
            benchmarks/.venv
          key: uv-${{ runner.os }}-${{ hashFiles('uv.lock', 'benchmarks/uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-

      - name: Cache APT packages
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: tesseract-ocr tesseract-ocr-eng tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-heb tesseract-ocr-chi-sim tesseract-ocr-jpn tesseract-ocr-kor poppler-utils libmagic1 pandoc
          version: 2.0

      - name: Install Python Dependencies
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 5
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            echo "Installing entire workspace with all packages and extras..."
            # Install everything for benchmarks - all extras, all packages, all groups
            uv sync --all-extras --all-packages --all-groups
          shell: bash

      - name: Clear Kreuzberg Cache
        run: |
          echo "Clearing any existing Kreuzberg cache..."
          rm -rf ~/.kreuzberg .kreuzberg benchmarks/.kreuzberg
          echo "Cache cleared"

      - name: Run Framework Benchmark
        id: benchmark
        run: |
          cd benchmarks

          ITERATIONS="${{ github.event.inputs.iterations }}"
          TIMEOUT="${{ github.event.inputs.timeout }}"
          FRAMEWORK="${{ matrix.framework }}"

          echo "=== Starting Benchmark for $FRAMEWORK ==="
          echo "Framework: $FRAMEWORK"
          echo "Iterations: $ITERATIONS"
          echo "Timeout: ${TIMEOUT}s per file"

          # Create results directory for this framework
          mkdir -p results/$FRAMEWORK

          # Run benchmarks for specific framework
          uv run python -m src.cli \
            --iterations "$ITERATIONS" \
            --timeout "$TIMEOUT" \
            --framework "$FRAMEWORK" \
            --output "results/$FRAMEWORK/results.json"

          echo "Benchmark execution completed for $FRAMEWORK"

      - name: Upload Framework Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.framework }}-${{ github.run_id }}
          path: benchmarks/results/${{ matrix.framework }}/
          retention-days: 90

  aggregate-results:
    # Always run aggregation, even if some frameworks fail
    # This allows partial results to be collected and analyzed
    if: ${{ always() && github.event_name == 'workflow_dispatch' }}
    needs: framework-benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-extras --all-packages --all-groups

      - name: Download All Framework Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*-${{ github.run_id }}
          path: benchmarks/individual-results/
          merge-multiple: true

      - name: Aggregate Results
        run: |
          cd benchmarks
          echo "=== Aggregating Results from All Frameworks ==="

          mkdir -p aggregated-results

          # Find all individual result files
          find individual-results -name "results.json" -type f | while read file; do
            framework=$(basename $(dirname "$file"))
            echo "Found results for framework: $framework"
          done

          # Run aggregation script
          uv run python -c "
          import json
          import glob
          from pathlib import Path

          all_results = []

          # Collect all individual results
          for result_file in glob.glob('individual-results/*/results.json'):
              try:
                  with open(result_file) as f:
                      data = json.load(f)
                      if isinstance(data, list):
                          all_results.extend(data)
                      else:
                          all_results.append(data)
              except Exception as e:
                  print(f'Error reading {result_file}: {e}')

          # Save aggregated results
          with open('aggregated-results/aggregated_results.json', 'w') as f:
              json.dump(all_results, f, indent=2)

          print(f'Aggregated {len(all_results)} results')
          "

      - name: Generate Visualizations
        if: always()
        run: |
          echo "=== Generating Visualizations ==="
          mkdir -p docs/benchmarks/charts

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_visualizations.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks/charts

            echo "âœ… Visualizations generated successfully"
          else
            echo "âš ï¸ No aggregated results for visualization"
          fi

      - name: Generate Documentation
        if: always()
        run: |
          echo "=== Generating Benchmark Documentation ==="

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_docs.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks \
              --charts docs/benchmarks/charts

            echo "âœ… Documentation generated successfully"
          else
            echo "âš ï¸ No aggregated results for documentation"
          fi

      - name: Create Summary
        if: always()
        run: |
          echo "## ðŸ“Š Comparative Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Iterations**: ${{ github.event.inputs.iterations }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timeout**: ${{ github.event.inputs.timeout }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add summary from aggregated results if available
          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            echo "### Results Summary" >> $GITHUB_STEP_SUMMARY
            python -c 'import json; data=json.load(open("benchmarks/aggregated-results/aggregated_results.json")); print(f"- **Total Results**: {len(data) if isinstance(data, list) else 1}") if "error" not in str(data) else print("âŒ Error in results")' >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "âš ï¸ Could not parse results" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No aggregated results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Aggregated Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-aggregated-${{ github.run_id }}
          path: benchmarks/aggregated-results/
          retention-days: 90

      - name: Upload Documentation
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-docs-${{ github.run_id }}
          path: docs/benchmarks/
          retention-days: 90
