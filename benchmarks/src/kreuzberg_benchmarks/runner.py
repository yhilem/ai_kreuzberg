"""Benchmark runner and execution engine."""

from __future__ import annotations

import asyncio
import time
import traceback
from typing import TYPE_CHECKING, Any, Callable, Sequence

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
from rich.table import Table

from .models import (
    BenchmarkResult,
    BenchmarkSuite,
    FlameGraphConfig,
    PerformanceMetrics,
    SystemInfo,
)
from .profiler import PerformanceProfiler

if TYPE_CHECKING:
    from pathlib import Path


class BenchmarkRunner:
    """Executes and manages benchmark suites."""

    def __init__(
        self,
        console: Console | None = None,
        flame_config: FlameGraphConfig | None = None,
    ) -> None:
        self.console = console or Console()
        self.flame_config = flame_config or FlameGraphConfig()
        self.system_info = SystemInfo.collect()

    def run_sync_benchmark(
        self,
        name: str,
        func: Callable[..., Any],
        metadata: dict[str, Any] | None = None,
    ) -> BenchmarkResult:
        """Run a single synchronous benchmark."""
        profiler = PerformanceProfiler()

        try:
            profiler.start_monitoring()
            start_time = time.perf_counter()

            # Execute the benchmark function
            func()

            end_time = time.perf_counter()
            performance_metrics = profiler.stop_monitoring()

            # Update duration with more precise timing
            performance_metrics.duration_seconds = end_time - start_time

            return BenchmarkResult(
                name=name,
                success=True,
                performance=performance_metrics,
                metadata=metadata or {},
            )

        except Exception as e:
            # Stop profiling and create failed result
            try:
                performance_metrics = profiler.stop_monitoring()
                performance_metrics.exception_info = str(e)
            except:
                performance_metrics = PerformanceMetrics(
                    duration_seconds=0.0,
                    memory_peak_mb=0.0,
                    memory_average_mb=0.0,
                    cpu_percent_average=0.0,
                    cpu_percent_peak=0.0,
                    gc_collections={},
                    exception_info=str(e),
                )

            return BenchmarkResult(
                name=name,
                success=False,
                performance=performance_metrics,
                metadata={
                    **(metadata or {}),
                    "error_traceback": traceback.format_exc(),
                },
            )

    async def run_async_benchmark(
        self,
        name: str,
        func: Callable[..., Any],
        metadata: dict[str, Any] | None = None,
    ) -> BenchmarkResult:
        """Run a single asynchronous benchmark."""
        profiler = PerformanceProfiler()

        try:
            profiler.start_monitoring()
            start_time = time.perf_counter()

            # Execute the async benchmark function
            if asyncio.iscoroutinefunction(func):
                await func()
            else:
                func()

            end_time = time.perf_counter()
            performance_metrics = profiler.stop_monitoring()

            # Update duration with more precise timing
            performance_metrics.duration_seconds = end_time - start_time

            return BenchmarkResult(
                name=name,
                success=True,
                performance=performance_metrics,
                metadata=metadata or {},
            )

        except Exception as e:
            # Stop profiling and create failed result
            try:
                performance_metrics = profiler.stop_monitoring()
                performance_metrics.exception_info = str(e)
            except:
                performance_metrics = PerformanceMetrics(
                    duration_seconds=0.0,
                    memory_peak_mb=0.0,
                    memory_average_mb=0.0,
                    cpu_percent_average=0.0,
                    cpu_percent_peak=0.0,
                    gc_collections={},
                    exception_info=str(e),
                )

            return BenchmarkResult(
                name=name,
                success=False,
                performance=performance_metrics,
                metadata={
                    **(metadata or {}),
                    "error_traceback": traceback.format_exc(),
                },
            )

    def run_benchmark_suite(
        self,
        suite_name: str,
        benchmarks: Sequence[tuple[str, Callable[..., Any], dict[str, Any] | None]],
        async_benchmarks: Sequence[
            tuple[str, Callable[..., Any], dict[str, Any] | None]
        ]
        | None = None,
    ) -> BenchmarkSuite:
        """Run a complete benchmark suite with both sync and async tests."""
        start_time = time.perf_counter()
        results: list[BenchmarkResult] = []

        total_benchmarks = len(benchmarks) + (
            len(async_benchmarks) if async_benchmarks else 0
        )

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            TimeElapsedColumn(),
            console=self.console,
        ) as progress:
            task = progress.add_task(f"Running {suite_name}", total=total_benchmarks)

            # Run synchronous benchmarks
            for name, func, metadata in benchmarks:
                progress.update(task, description=f"Running sync: {name}")
                result = self.run_sync_benchmark(name, func, metadata)
                results.append(result)
                progress.advance(task)

                # Log result
                status = "✓" if result.success else "✗"
                duration = (
                    result.performance.duration_seconds if result.performance else 0
                )
                self.console.print(f"  {status} {name}: {duration:.3f}s")

            # Run asynchronous benchmarks
            if async_benchmarks:

                async def run_async_suite() -> list[BenchmarkResult]:
                    async_results = []
                    for name, func, metadata in async_benchmarks:
                        progress.update(task, description=f"Running async: {name}")
                        result = await self.run_async_benchmark(name, func, metadata)
                        async_results.append(result)
                        progress.advance(task)

                        # Log result
                        status = "✓" if result.success else "✗"
                        duration = (
                            result.performance.duration_seconds
                            if result.performance
                            else 0
                        )
                        self.console.print(f"  {status} {name}: {duration:.3f}s")

                    return async_results

                async_results = asyncio.run(run_async_suite())
                results.extend(async_results)

        total_duration = time.perf_counter() - start_time

        return BenchmarkSuite(
            name=suite_name,
            system_info=self.system_info,
            results=results,
            total_duration_seconds=total_duration,
        )

    def print_summary(self, suite: BenchmarkSuite) -> None:
        """Print a summary table of benchmark results."""
        table = Table(title=f"Benchmark Results: {suite.name}")
        table.add_column("Benchmark", style="cyan", no_wrap=True)
        table.add_column("Status", style="green", justify="center")
        table.add_column("Duration", style="magenta", justify="right")
        table.add_column("Memory Peak", style="blue", justify="right")
        table.add_column("CPU Avg", style="yellow", justify="right")

        for result in suite.results:
            status = "✓" if result.success else "✗"
            status_style = "green" if result.success else "red"

            if result.performance:
                duration = f"{result.performance.duration_seconds:.3f}s"
                memory = f"{result.performance.memory_peak_mb:.1f}MB"
                cpu = f"{result.performance.cpu_percent_average:.1f}%"

                if result.performance.exception_info:
                    duration += f" ({result.performance.exception_info[:30]}...)"
            else:
                duration = memory = cpu = "N/A"

            table.add_row(
                result.name,
                f"[{status_style}]{status}[/{status_style}]",
                duration,
                memory,
                cpu,
            )

        self.console.print(table)

        # Print summary statistics
        successful = suite.successful_results
        if successful:
            total_time = sum(
                r.performance.duration_seconds for r in successful if r.performance
            )
            avg_time = total_time / len(successful)
            max_memory = max(
                r.performance.memory_peak_mb for r in successful if r.performance
            )

            self.console.print("\n[bold]Summary:[/bold]")
            self.console.print(f"  Success Rate: {suite.success_rate:.1f}%")
            self.console.print(f"  Total Time: {total_time:.3f}s")
            self.console.print(f"  Average Time: {avg_time:.3f}s")
            self.console.print(f"  Peak Memory: {max_memory:.1f}MB")
            self.console.print(
                f"  System: {suite.system_info.machine} ({suite.system_info.cpu_count} cores)"
            )

    def save_results(self, suite: BenchmarkSuite, output_path: Path) -> None:
        """Save benchmark results to JSON file."""
        import json

        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w") as f:
            json.dump(suite.to_dict(), f, indent=2, default=str)

        self.console.print(f"\n[green]Results saved to:[/green] {output_path}")
